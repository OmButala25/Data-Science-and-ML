{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8f6939c5-579d-4928-bfeb-32fd00c85212",
      "metadata": {},
      "source": [
        "# Pyspark Tutorial: Getting Started with Pyspark\n",
        "\n",
        "Discover what Pyspark is and how it can be used while giving examples."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3b3e013-3fce-4889-afeb-4760a62846e2",
      "metadata": {},
      "source": [
        "## An Introduction to Apache Spark\n",
        "\n",
        "[Apache Spark](https://spark.apache.org) is a distributed processing system used to perform big data and machine learning tasks on large datasets.\n",
        "\n",
        "As a data science enthusiast, you are probably familiar with storing files on your local device and processing it using languages like R and Python. However, local workstations have their limitations and cannot handle extremely large datasets. \n",
        "\n",
        "This is where a distributed processing system like Apache Spark comes in. Distributed processing is a setup in which multiple processors are used to run an application. Instead of trying to process large datasets on a single computer, the task can be divided between multiple devices that communicate with each other.\n",
        "\n",
        "With Apache Spark, users can run queries and machine learning workflows on petabytes of data, which is impossible to do on your local device.\n",
        "\n",
        "This framework is even faster than previous data processing engines like Hadoop, and has [increased in popularity](https://datafloq.com/read/why-apache-spark-becoming-more-popular) in the past eight years. Companies like IBM, Amazon, and Yahoo are using Apache Spark as their computational framework.\n",
        "\n",
        "The ability to analyze data and train machine learning models on large-scale datasets is a valuable skill to have if you want to become a data scientist. Having the expertise to work with big data frameworks like Apache Spark will set you apart from others in the field.\n",
        "\n",
        "_**Practice using Pyspark with hands-on exercises in our [Introduction to PySpark course](https://www.datacamp.com/courses/introduction-to-pyspark).**_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90f7d333-4330-4f16-acdb-ec19c753fdf4",
      "metadata": {},
      "source": [
        "## What is PySpark?\n",
        "\n",
        "PySpark is an interface for Apache Spark in Python. With PySpark, you can write Python and SQL-like commands to manipulate and analyze data in a distributed processing environment. To learn the basics of the language, you can take Datacamp’s [Introduction to PySpark](https://www.datacamp.com/courses/introduction-to-pyspark) course. This is a beginner program that will take you through manipulating data, building machine learning pipelines, and tuning models with PySpark.\n",
        "\n",
        "## What is PySpark used for?\n",
        "\n",
        "Most data scientists and analysts are familiar with Python and use it to implement machine learning workflows. PySpark allows them to work with a familiar language on large-scale distributed datasets.\n",
        "\n",
        "Apache Spark can also be used with other data science programming languages like R. If this is something you are interested in learning, the[ Introduction to Spark with sparklyr in R](https://www.datacamp.com/courses/introduction-to-spark-with-sparklyr-in-r) course is a great place to start.\n",
        "\n",
        "## Why PySpark?\n",
        "\n",
        "Companies that collect terabytes of data will have a big data framework like Apache Spark in place. To work with these large-scale datasets, knowledge of Python and R frameworks alone will not suffice. \n",
        "\n",
        "You need to learn a framework that allows you to manipulate datasets on top of a distributed processing system, as most data-driven organizations will require you to do so. PySpark is a great place to get started, since its syntax is simple and can be picked up easily if you are already familiar with Python.\n",
        "\n",
        "The reason companies choose to use a framework like PySpark is because of how quickly it can process big data. It is faster than libraries like Pandas and Dask, and can handle larger amounts of data than these frameworks. If you had over petabytes of data to process, for instance, Pandas and Dask would fail but PySpark would be able to handle it easily.\n",
        "\n",
        "While it is also possible to write Python code on top of a distributed system like Hadoop, many organizations choose to use Spark instead and use the PySpark API since it is faster and can handle real-time data. With PySpark, you can write code to collect data from a source that is continuously updated, while data can only be processed in batch mode with Hadoop. \n",
        "\n",
        "Apache Flink is a distributed processing system that has a Python API called PyFlink, and is actually faster than Spark in terms of performance. However, Apache Spark has been around for a longer period of time and has better community support, which means that it is more reliable. \n",
        "\n",
        "Furthermore, PySpark provides fault tolerance, which means that it has the capability to recover loss after a failure occurs. The framework also has in-memory computation and is stored in random access memory (RAM). It can run on a machine that does not have a hard-drive or SSD installed.\n",
        "\n",
        "### Dataset\n",
        "\n",
        "We will be using Datacamp’s [e-commerce dataset](https://www.datacamp.com/workspace/datasets/dataset-r-e-commerce) for all the analysis in this tutorial, so make sure to have it downloaded. We’ve  renamed the file to “datacamp_ecommerce.csv” and saved it to the  parent directory, and you can do the same so it’s easier to code along.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8fc7ff5-c898-4881-9878-a26bcce6fa3f",
      "metadata": {},
      "source": [
        "## End-to-end Machine Learning PySpark Tutorial\n",
        "\n",
        "Now that you have PySpark up and running, we will show you how to execute an end-to-end customer segmentation project using the library. \n",
        "\n",
        "Customer segmentation is a marketing technique companies use  to identify and group users who display similar characteristics. For instance, if you visit Starbucks only during the summer to purchase cold beverages, you can be segmented as a “seasonal shopper” and enticed with special promotions curated for  the summer season.\n",
        "\n",
        "Data scientists usually build unsupervised machine learning algorithms such as K-Means clustering or hierarchical clustering to perform customer segmentation. These models are great at identifying similar patterns between user groups that often go unnoticed by the human eye.\n",
        "\n",
        "In this tutorial, we will use K-Means clustering to perform customer segmentation on the e-commerce dataset we downloaded earlier.\n",
        "\n",
        "By the end of this tutorial, you will be familiar with the following concepts:\n",
        "\n",
        "- Reading csv files with PySpark\n",
        "- Exploratory Data Analysis with PySpark\n",
        "- Grouping and sorting data\n",
        "- Performing arithmetic operations\n",
        "- Aggregating datasets\n",
        "- Data Pre-Processing with PySpark\n",
        "- Working with datetime values\n",
        "- Type conversion\n",
        "- Joining two dataframes\n",
        "- The rank() function\n",
        "- PySpark Machine Learning\n",
        "- Creating a feature vector\n",
        "- Standardizing data\n",
        "- Building a K-Means clustering model\n",
        "- Interpreting the model\n",
        "\n",
        "\n",
        "### Step 1: Creating a SparkSession\n",
        "\n",
        "A SparkSession is an entry point into all functionality in Spark, and is required if you want to build a dataframe in PySpark. Run the following lines of code to initialize a SparkSession:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "628cedac-4914-4808-af24-2bc3c70cce06",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "dd93cede-d7d8-4a7c-a00f-0c23207ca280",
      "metadata": {},
      "outputs": [
        {
          "ename": "PySparkRuntimeError",
          "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# spark = SparkSession.builder.appName(\"Datacamp Pyspark Tutorial\").config(\"spark.memory.offHeap.enabled\",\"true\").config(\"spark.memory.offHeap.size\",\"10g\").getOrCreate()\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# spark = SparkSession.builder.appName(\"Datacamp Pyspark Tutorial\").getOrCreate()\u001b[39;00m\n\u001b[0;32m      5\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPython Spark SQL basic example\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.some.config.option\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msome-value\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m----> 9\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pyspark\\sql\\session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pyspark\\context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pyspark\\context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m     )\n\u001b[1;32m--> 201\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[0;32m    204\u001b[0m         master,\n\u001b[0;32m    205\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    216\u001b[0m     )\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pyspark\\context.py:436\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[1;32m--> 436\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    437\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pyspark\\java_gateway.py:107\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[0;32m    108\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    109\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[0;32m    110\u001b[0m     )\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[0;32m    113\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
            "\u001b[1;31mPySparkRuntimeError\u001b[0m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
          ]
        }
      ],
      "source": [
        "# spark = SparkSession.builder.appName(\"Datacamp Pyspark Tutorial\").config(\"spark.memory.offHeap.enabled\",\"true\").config(\"spark.memory.offHeap.size\",\"10g\").getOrCreate()\n",
        "\n",
        "# spark = SparkSession.builder.appName(\"Datacamp Pyspark Tutorial\").getOrCreate()\n",
        "\n",
        "spark = SparkSession \\\n",
        " .builder \\\n",
        " .appName(\"Python Spark SQL basic example\") \\\n",
        " .config(\"spark.some.config.option\", \"some-value\") \\\n",
        " .getOrCreate()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "670b97ed-26d6-49e3-9555-03fea7ba8106",
      "metadata": {},
      "source": [
        "Using the codes above, we built a spark session and set a name for the application. Then, the data was cached in off-heap memory to avoid storing it directly on disk, and the amount of memory was manually specified.\n",
        "\n",
        "### Step 2: Creating the DataFrame\n",
        "\n",
        "We can now read the dataset we just downloaded:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "497e023a-8b78-4778-a198-64883c1cac73",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = spark.read.csv('data/online_retail.csv',header=True,escape=\"\\\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b96f43a-dbb2-4c46-9568-b2fc25bea7e1",
      "metadata": {},
      "source": [
        "Note that we defined an escape character to avoid commas in the .csv file when parsing.\n",
        "\n",
        "Let’s take a look at the head of the dataframe using the show() function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5445be21-179a-4c01-9e28-8f7c7bb6a8a2",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.show(5,0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d1af8cd-71eb-435f-a21c-a365521ae651",
      "metadata": {},
      "source": [
        "The dataframe consists of 8 variables:\n",
        "\n",
        "- InvoiceNo: The unique identifier of each customer invoice.\n",
        "- StockCode: The unique identifier of each item in stock.\n",
        "- Description: The item purchased by the customer.\n",
        "- Quantity: The number of each item purchased by a customer in a single invoice.\n",
        "- InvoiceDate: The purchase date.\n",
        "- UnitPrice: Price of one unit of each item.\n",
        "- CustomerID: Unique identifier assigned to each user.\n",
        "- Country: The country from where the purchase was made\n",
        "\n",
        "\n",
        "### Step 3: Exploratory Data Analysis\n",
        "\n",
        "Now that we have seen the variables present in this dataset, let’s perform some exploratory data analysis to further understand these data points:\n",
        "\n",
        "- Let’s start by counting the number of rows in the dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82c2cb1f-e653-415e-8073-5b1688b2595d",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.count()  # Answer: 541909"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b37cd7c2-fd61-4d49-93d9-1df6f8010376",
      "metadata": {},
      "source": [
        "- How many unique customers are present in the dataframe?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1980c0bf-da36-409f-80bf-08e49b28fecc",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.select('CustomerID').distinct().count() # Answer: 4373"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8259eb05-5b86-4ca4-b564-cea406e45c30",
      "metadata": {},
      "source": [
        "- What country do most purchases come from?\n",
        "\n",
        "\n",
        "To find the country from which most purchases are made, we need to use the groupBy() clause in PySpark:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d28f04e3-e72e-4d74-a2cb-f52e1e45b798",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.groupBy('Country').agg(countDistinct('CustomerID').alias('country_count')).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c11f9ed-22a1-4dac-adaa-9df77b3bb24e",
      "metadata": {},
      "source": [
        "The following table will be rendered after running the codes above:\n",
        "\n",
        "\n",
        "\n",
        "Almost all the purchases on the platform were made from the United Kingdom, and only a handful were made from countries like Germany, Australia, and France. \n",
        "\n",
        "Notice that the data in the table above isn’t presented in the order of purchases. To sort this table, we can include the orderBy() clause:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cccf313-1825-4285-92e8-e86dadead724",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.groupBy('Country').agg(countDistinct('CustomerID').alias('country_count')).orderBy(desc('country_count')).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd49fc3a-6daa-4921-9773-b6d496d020b1",
      "metadata": {},
      "source": [
        "The output displayed is now sorted in descending order:\n",
        "\n",
        "\n",
        "\n",
        "- When was the most recent purchase made by a customer on the e-commerce platform?\n",
        "\n",
        "\n",
        "To find when the latest purchase was made on the platform, we need to convert the “InvoiceDate” column into a timestamp format and use the max() function in Pyspark:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd026ec2-786b-459c-a45b-543f4978c066",
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
        "df = df.withColumn('date',to_timestamp(\"InvoiceDate\", 'yy/MM/dd HH:mm'))\n",
        "df.select(max(\"date\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea5cb3a2-1853-4506-a74e-a7a595da3e62",
      "metadata": {},
      "source": [
        "You should see the following table appear after running the code above:\n",
        "\n",
        "\n",
        "\n",
        "- When was the earliest purchase made by a customer on the e-commerce platform?\n",
        "\n",
        "\n",
        "Similar to what we did above, the min() function can be used to find the earliest purchase date and time:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb84704f-7780-4cc1-a1d5-20fe2b282cfe",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.select(min(\"date\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4611a722-4065-4edd-98a3-82975da160e1",
      "metadata": {},
      "source": [
        "Notice that the most recent and earliest purchases were made on the same day just a few hours apart. This means that the dataset we downloaded contains information of only purchases made on a single day.\n",
        "\n",
        "### Step 4: Data Pre-processing\n",
        "\n",
        "Now that we have analyzed the dataset and have a better understanding of each data point, we need to prepare the data to feed into the machine learning algorithm.\n",
        "\n",
        "Let’s take a look at the head of the dataframe once again to understand how the pre-processing will be done:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e6b5360-a72d-4e50-b116-dbeda082c923",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.show(5,0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b54cc90e-d1b2-4e71-b293-d3f6e566f828",
      "metadata": {},
      "source": [
        "From the dataset above, we need to create multiple customer segments based on each user’s purchase behavior. \n",
        "\n",
        "The variables in this dataset are in a format that cannot be easily ingested into the customer segmentation model. These features individually do not tell us much about customer purchase behavior.\n",
        "\n",
        "Due to this, we will use the existing variables to derive three new informative features - recency, frequency, and monetary value (RFM).\n",
        "\n",
        "[RFM](https://www.investopedia.com/terms/r/rfm-recency-frequency-monetary-value.asp) is commonly used in marketing to evaluate a client’s value based on their:\n",
        "\n",
        "- Recency: How recently has each customer made a purchase?\n",
        "- Frequency: How often have they bought something?\n",
        "- Monetary Value: How much money do they spend on average when making purchases?\n",
        "\n",
        "\n",
        "We will now preprocess the dataframe to create the above variables.\n",
        "\n",
        "#### Recency\n",
        "\n",
        "First, let’s calculate the value of recency - the latest date and time a purchase was made on the platform. This can be achieved in two steps:\n",
        "\n",
        "##### i) Assign a recency score to each customer\n",
        "\n",
        "We will subtract every date in the dataframe from the earliest date. This will tell us how recently a customer was seen in the dataframe. A value of 0 indicates the lowest recency, as it will be assigned to the person who was seen making a purchase on the earliest date."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ae4dbdb-7260-4e62-a21a-3dcf6404356a",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df.withColumn(\"from_date\", lit(\"12/1/10 08:26\"))\n",
        "df = df.withColumn('from_date',to_timestamp(\"from_date\", 'yy/MM/dd HH:mm'))\n",
        "\n",
        "df2=df.withColumn('from_date',to_timestamp(col('from_date'))).withColumn('recency',col(\"date\").cast(\"long\") - col('from_date').cast(\"long\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45c96156-7347-4417-b435-5f037fb4d8c7",
      "metadata": {},
      "source": [
        "##### ii) Select the most recent purchase\n",
        "\n",
        "One customer can make multiple purchases at different times. We need to select only the last time they were seen buying a product, as this is indicative of when the most recent purchase was made:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36d7994d-a75d-4983-82ad-18a1e0d77c92",
      "metadata": {},
      "outputs": [],
      "source": [
        "df2 = df2.join(df2.groupBy('CustomerID').agg(max('recency').alias('recency')),on='recency',how='leftsemi')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d67dc75e-1044-4523-802d-f1d57e127c89",
      "metadata": {},
      "source": [
        "Let’s look at the head of the new dataframe. It now has a variable called “recency” appended to it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68e3319d-2cb4-4fba-b84e-543f96571837",
      "metadata": {},
      "outputs": [],
      "source": [
        "df2.show(5,0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c046689f-05e7-4a3f-8cbc-d8f536646f33",
      "metadata": {},
      "source": [
        "An easier way to view all the variables present in a PySpark dataframe is to use its printSchema() function. This is the equivalent of the info() function in Pandas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5c9c05f-d419-405e-bd30-7e815658adf9",
      "metadata": {},
      "outputs": [],
      "source": [
        "df2.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f043468-d004-4fda-afdc-e91e499fa631",
      "metadata": {},
      "source": [
        "The output rendered should look like this:\n",
        "\n",
        "\n",
        "\n",
        "#### Frequency\n",
        "\n",
        "Let’s now calculate the value of frequency - how often a customer bought something on the platform. To do this, we just need to group by each customer ID and count the number of items they purchased:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44ae0a9f-2112-4655-b963-5a57c60d233b",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_freq = df2.groupBy('CustomerID').agg(count('InvoiceDate').alias('frequency'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e4143e7-d6c6-4fdd-878b-b86d694757ea",
      "metadata": {},
      "source": [
        "Look at the head of this new dataframe we just created:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34d3e2bb-9e8c-414f-86b4-2274914ab5d0",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_freq.show(5,0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a27c3eee-a81d-487d-ba0b-c35aad077036",
      "metadata": {},
      "source": [
        "There is a frequency value appended to each customer in the dataframe. This new dataframe only has two columns, and we need to join it with the previous one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "453448bc-93ed-4b6f-baaf-a0d8aadc3dc4",
      "metadata": {},
      "outputs": [],
      "source": [
        "df3 = df2.join(df_freq,on='CustomerID',how='inner')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9fc0908-8982-4779-a40d-8ca88ba42111",
      "metadata": {},
      "source": [
        "Let’s print the schema of this dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad821ee6-581b-433b-9ee8-2caede7c3474",
      "metadata": {},
      "outputs": [],
      "source": [
        "df3.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8155167b-d1bc-48a6-abcd-93be36c5ff46",
      "metadata": {},
      "source": [
        "#### \n",
        "\n",
        "#### Monetary Value\n",
        "\n",
        "Finally, let’s calculate monetary value - the total amount spent by each customer in the dataframe. There are two steps to achieving this:\n",
        "\n",
        "##### i) Find the total amount spent in each purchase:\n",
        "\n",
        "Each customerID comes with variables called “Quantity” and “UnitPrice” for a single purchase:\n",
        "\n",
        "\n",
        "\n",
        "To get the total amount spent by each customer in one purchase, we need to multiply “Quantity” with “UnitPrice”:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ba50841-e35b-4f29-92b9-d35ec8fc440e",
      "metadata": {},
      "outputs": [],
      "source": [
        "m_val = df3.withColumn('TotalAmount',col(\"Quantity\") * col(\"UnitPrice\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddb76833-dbe4-4929-95e5-10f9b12c3b63",
      "metadata": {},
      "source": [
        "##### ii) Find the total amount spent by each customer:\n",
        "\n",
        "To find the total amount spent by each customer overall, we just need to group by the CustomerID column and sum the total amount spent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88a04c60-8107-4d7e-9753-5905df73a0d0",
      "metadata": {},
      "outputs": [],
      "source": [
        "m_val = m_val.groupBy('CustomerID').agg(sum('TotalAmount').alias('monetary_value'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4945960e-4707-4b94-8e7d-7d530fae915c",
      "metadata": {},
      "source": [
        "Merge this dataframe with the all the other variables:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78ca3a53-64e6-4aba-bfa5-fa20336f96b4",
      "metadata": {},
      "outputs": [],
      "source": [
        "finaldf = m_val.join(df3,on='CustomerID',how='inner')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75d7c2ec-7966-4a9a-9aaa-83fd17cf5cf6",
      "metadata": {},
      "source": [
        "Now that we have created all the necessary variables to build the model, run the following lines of code to select only the required columns and drop duplicate rows from the dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96548be6-122b-4653-98e3-e1c5f3e40549",
      "metadata": {},
      "outputs": [],
      "source": [
        "finaldf = finaldf.select(['recency','frequency','monetary_value','CustomerID']).distinct()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20b94777-57eb-41f4-82df-28ebb636c2b2",
      "metadata": {},
      "source": [
        "Look at the head of the final dataframe to ensure that the pre-processing has been done accurately:\n",
        "\n",
        "\n",
        "\n",
        "#### Standardization\n",
        "\n",
        "Before building the customer segmentation model, let’s standardize the dataframe to ensure that all the variables are around the same scale:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fa27200-b023-42e2-a938-c17c58b4b751",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "\n",
        "assemble=VectorAssembler(inputCols=[\n",
        "    'recency','frequency','monetary_value'\n",
        "], outputCol='features')\n",
        "\n",
        "assembled_data=assemble.transform(finaldf)\n",
        "\n",
        "scale=StandardScaler(inputCol='features',outputCol='standardized')\n",
        "data_scale=scale.fit(assembled_data)\n",
        "data_scale_output=data_scale.transform(assembled_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "906a90db-335e-4e80-9768-64771d6c7e1d",
      "metadata": {},
      "source": [
        "Run the following lines of code to see what the standardized feature vector looks like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "588d9e01-0bad-45bf-bd8a-734d96db103b",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_scale_output.select('standardized').show(2,truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f274e5b-1350-47ca-8cc3-2973df9427bd",
      "metadata": {},
      "source": [
        "These are the scaled features that will be fed into the clustering algorithm.\n",
        "\n",
        "If you’d like to learn more about data preparation with PySpark, take this [feature engineering course](https://www.datacamp.com/courses/feature-engineering-with-pyspark) on Datacamp.\n",
        "\n",
        "### Step 5: Building the Machine Learning Model\n",
        "\n",
        "Now that we have completed all the data analysis and preparation, let’s build the K-Means clustering model. \n",
        "\n",
        "The algorithm will be created using PySpark’s [machine learning API](https://spark.apache.org/docs/2.3.0/api/python/pyspark.ml.html).\n",
        "\n",
        "#### i) Finding the number of clusters to use\n",
        "\n",
        "When building a K-Means clustering model, we first need to determine the number of clusters or groups we want the algorithm to return. If we decide on three clusters, for instance, then we will have three customer segments.\n",
        "\n",
        "The most popular technique used to decide on how many clusters to use in K-Means is called the “elbow-method.”\n",
        "\n",
        "This is done simply running the K-Means algorithm for a wide range of clusters and visualizing the model results for each cluster. The plot will have an inflection point that looks like an elbow, and we just pick the number of clusters at this point.\n",
        "\n",
        "Read this Datacamp [K-Means clustering tutorial](https://www.datacamp.com/tutorial/k-means-clustering-python) to learn more about how the algorithm works.\n",
        "\n",
        "Let’s run the following lines of code to build a K-Means clustering algorithm from 2 to 6 clusters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "291ff579-28cd-41a4-ab43-ba136af3be2d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "\n",
        "max_k = 6\n",
        "\n",
        "cost = np.zeros(max_k)\n",
        "\n",
        "evaluator = ClusteringEvaluator(predictionCol='prediction', featuresCol='standardized',metricName='silhouette', distanceMeasure='squaredEuclidean')\n",
        "\n",
        "for i in range(2, max_k):\n",
        "    KMeans_algo=KMeans(featuresCol='standardized', k=i)\n",
        "    KMeans_fit=KMeans_algo.fit(data_scale_output)\n",
        "    output=KMeans_fit.transform(data_scale_output)\n",
        "    cost[i] = KMeans_fit.summary.trainingCost"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e81a9f5-be88-4bb6-94cf-bb7c486f6aad",
      "metadata": {},
      "source": [
        "With the codes above, we have successfully built and evaluated a K-Means clustering model with 2 to 6 clusters. The results have been placed in an array, and can now be visualized in a line chart:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f7ed735-d289-4ded-bf4a-f3f7c8736dc0",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_cost = pd.DataFrame(cost[2:])\n",
        "df_cost.columns = [\"cost\"]\n",
        "new_col = range(2, max_k)\n",
        "df_cost.insert(0, 'cluster', new_col)\n",
        "plt.plot(df_cost.cluster, df_cost.cost)\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Elbow Curve')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9d70581-0cb6-475d-8f1d-79a005884c18",
      "metadata": {},
      "source": [
        "The codes above will render the following chart:\n",
        "\n",
        "\n",
        "\n",
        "#### ii) Building the K-Means Clustering Model\n",
        "\n",
        "From the plot above, we can see that there is an inflection point that looks like an elbow at four. Due to this, we will proceed to build the K-Means algorithm with four clusters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e532bc84-8eb3-45b6-9c80-5b60c991a53c",
      "metadata": {},
      "outputs": [],
      "source": [
        "KMeans_algo=KMeans(featuresCol='standardized', k=4)\n",
        "KMeans_fit=KMeans_algo.fit(data_scale_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c391f83-642e-4bd1-bf04-59a8aab35348",
      "metadata": {},
      "source": [
        "#### iii) Making Predictions\n",
        "\n",
        "Let’s use the model we created to assign clusters to each customer in the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "075fdf1d-22d7-41b2-b5e5-efbd7ca6bb7c",
      "metadata": {},
      "outputs": [],
      "source": [
        "preds=KMeans_fit.transform(data_scale_output)\n",
        "\n",
        "preds.show(5,0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f8759fa-eab5-489c-a624-19b221b110fd",
      "metadata": {},
      "source": [
        "Notice that there is a “prediction” column in this dataframe that tells us which cluster each CustomerID belongs to:\n",
        "\n",
        "\n",
        "\n",
        "### Step 6: Cluster Analysis\n",
        "\n",
        "The final step in this entire tutorial is to analyze the customer segments we just built.\n",
        "\n",
        "Run the following lines of code to visualize the recency, frequency, and monetary value of each customerID in the dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b6df561-bed0-428c-b03a-2018c9519548",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_viz = preds.select('recency','frequency','monetary_value','prediction')\n",
        "df_viz = df_viz.toPandas()\n",
        "avg_df = df_viz.groupby(['prediction'], as_index=False).mean()\n",
        "\n",
        "list1 = ['recency','frequency','monetary_value']\n",
        "\n",
        "for i in list1:\n",
        "    sns.barplot(x='prediction',y=str(i),data=avg_df)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9d93035-cb38-4b7e-81e0-5936436d0f1a",
      "metadata": {},
      "source": [
        "The codes above will render the following plots:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Here is an overview of characteristics displayed by customers in each cluster:\n",
        "\n",
        "- Cluster 0: Customers in this segment display low recency, frequency, and monetary value. They rarely shop on the platform and are low potential customers who are likely to stop doing business with the ecommerce company.\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "- Cluster 1: Users in this cluster display high recency but haven’t been seen spending much on the platform. They also don’t visit the site often. This indicates that they might be newer customers who have just started doing business with the company.\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "- Cluster 2: Customers in this segment display medium recency and frequency and spend a lot of money on the platform. This indicates that they tend to buy high-value items or make bulk purchases.\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "- Cluster 3: The final segment comprises users who display high recency and make frequent purchases on the platform. However, they don’t spend much on the platform, which might mean that they tend to select cheaper items in each purchase.\n",
        "\n",
        "\n",
        "To go beyond the predictive modelingmodelling concepts covered in this course, you can take the [Machine Learning with PySpark](https://www.datacamp.com/courses/machine-learning-with-pyspark) course on Datacamp.\n",
        "\n",
        "## Learning PySpark From Scratch - Next Steps:\n",
        "\n",
        "If you managed to follow along with this entire PySpark tutorial, congratulations! You have now successfully installed PySpark onto your local device, analyzed an e-commerce dataset, and built a machine learning algorithm using the framework.\n",
        "\n",
        "One caveat of the analysis above is that it was conducted with 2,500 rows of ecommerce data collected on a single day. The outcome of this analysis can be solidified if we had a larger amount of data to work with, as techniques like RFM modeling are usually applied onto months of historical data.\n",
        "\n",
        "However, you can take the principles learned  in this article and apply them to a wide variety of larger datasets in the unsupervised machine learning space.\n",
        "\n",
        "Check out this [cheat sheet](https://www.datacamp.com/cheat-sheet/pyspark-cheat-sheet-spark-in-python) by Datacamp to learn more about PySpark’s syntax and its modules.\n",
        "\n",
        "Finally, if you’d like to go beyond the concepts covered in this tutorial and learn the fundamentals of programming with PySpark, you can take the [Big Data with PySpark](https://www.datacamp.com/tracks/big-data-with-pyspark) learning track on Datacamp. This track contains a series of courses that will teach you to do the following with PySpark:\n",
        "\n",
        "- Data Management, Analysis, and Pre-processing\n",
        "- Building and Tuning Machine Learning Pipelines\n",
        "- Big Data Analysis \n",
        "- Feature Engineering \n",
        "- Building Recommendation Engines"
      ]
    }
  ],
  "metadata": {
    "editor": "DataLab",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
